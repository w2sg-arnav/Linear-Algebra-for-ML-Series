{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e961ee",
   "metadata": {},
   "source": [
    "# Interactive Tensor and Neural Network Operations Visualization\n",
    "\n",
    "This notebook provides interactive visualizations for understanding tensor operations and their applications in neural networks.\n",
    "\n",
    "## Contents\n",
    "1. Tensor Visualization Tools\n",
    "2. Neural Network Layer Operations\n",
    "3. Attention Mechanism Visualization\n",
    "4. Backpropagation Flow\n",
    "5. Interactive Layer Architecture Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff55de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "def create_custom_colormap(n_colors):\n",
    "    \"\"\"Create a custom colormap with good contrast\"\"\"\n",
    "    return sns.color_palette(\"husl\", n_colors=n_colors)\n",
    "\n",
    "class TensorVisualizer:\n",
    "    @staticmethod\n",
    "    def plot_tensor_3d(tensor, title=\"3D Tensor Visualization\"):\n",
    "        \"\"\"Create an interactive 3D visualization of a tensor\"\"\"\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        x, y, z = np.indices(tensor.shape)\n",
    "        values = tensor.flatten()\n",
    "        \n",
    "        # Normalize values for color mapping\n",
    "        norm_values = (values - values.min()) / (values.max() - values.min())\n",
    "        colors = plt.cm.viridis(norm_values)\n",
    "        \n",
    "        scatter = ax.scatter(x.flatten(), y.flatten(), z.flatten(),\n",
    "                           c=values, cmap='viridis', alpha=0.6)\n",
    "        \n",
    "        ax.set_xlabel('Dimension 1')\n",
    "        ax.set_ylabel('Dimension 2')\n",
    "        ax.set_zlabel('Dimension 3')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(title)\n",
    "        \n",
    "        return fig, ax\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_tensor_slices(tensor, axis=0):\n",
    "        \"\"\"Plot all slices of a tensor along a specified axis\"\"\"\n",
    "        n_slices = tensor.shape[axis]\n",
    "        n_cols = min(4, n_slices)\n",
    "        n_rows = (n_slices + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig = plt.figure(figsize=(15, 3*n_rows))\n",
    "        \n",
    "        for i in range(n_slices):\n",
    "            plt.subplot(n_rows, n_cols, i+1)\n",
    "            if axis == 0:\n",
    "                plt.imshow(tensor[i], cmap='viridis')\n",
    "            elif axis == 1:\n",
    "                plt.imshow(tensor[:, i], cmap='viridis')\n",
    "            else:\n",
    "                plt.imshow(tensor[:, :, i], cmap='viridis')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'Slice {i}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "class NeuralOpsVisualizer:\n",
    "    @staticmethod\n",
    "    def visualize_linear_layer(input_size, output_size, batch_size=1):\n",
    "        \"\"\"Interactive visualization of a linear layer transformation\"\"\"\n",
    "        weights = np.random.randn(input_size, output_size)\n",
    "        bias = np.random.randn(output_size)\n",
    "        \n",
    "        def update_plot(batch_idx):\n",
    "            input_data = np.random.randn(batch_size, input_size)\n",
    "            output_data = np.dot(input_data, weights) + bias\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Input visualization\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(input_data.reshape(-1, input_size), cmap='viridis')\n",
    "            plt.title('Input')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Weights visualization\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(weights, cmap='viridis')\n",
    "            plt.title('Weights')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Output visualization\n",
    "            plt.subplot(133)\n",
    "            plt.imshow(output_data.reshape(-1, output_size), cmap='viridis')\n",
    "            plt.title('Output')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        widgets.interact(update_plot, \n",
    "                        batch_idx=widgets.IntSlider(min=0, max=batch_size-1, \n",
    "                                                  step=1, value=0))\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_attention(seq_length, d_k):\n",
    "        \"\"\"Interactive visualization of self-attention mechanism\"\"\"\n",
    "        def compute_attention(temperature):\n",
    "            # Generate random queries, keys, and values\n",
    "            queries = np.random.randn(seq_length, d_k)\n",
    "            keys = np.random.randn(seq_length, d_k)\n",
    "            values = np.random.randn(seq_length, d_k)\n",
    "            \n",
    "            # Compute attention scores\n",
    "            scores = np.dot(queries, keys.T) / np.sqrt(d_k)\n",
    "            # Apply temperature scaling\n",
    "            scores = scores / temperature\n",
    "            # Compute attention weights\n",
    "            weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "            # Compute attention output\n",
    "            output = np.dot(weights, values)\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Attention weights\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(weights, cmap='viridis')\n",
    "            plt.title('Attention Weights')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Input values\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(values, cmap='viridis')\n",
    "            plt.title('Values')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Output\n",
    "            plt.subplot(133)\n",
    "            plt.imshow(output, cmap='viridis')\n",
    "            plt.title('Attention Output')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        widgets.interact(compute_attention, \n",
    "                        temperature=widgets.FloatSlider(min=0.1, max=2.0, \n",
    "                                                      step=0.1, value=1.0))\n",
    "\n",
    "class BackpropVisualizer:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = [np.random.randn(i, j) for i, j in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        self.gradients = [np.zeros_like(w) for w in self.weights]\n",
    "    \n",
    "    def visualize_network(self):\n",
    "        \"\"\"Visualize network architecture with gradients\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Plot neurons\n",
    "        for i, size in enumerate(self.layer_sizes):\n",
    "            x = np.ones(size) * i\n",
    "            y = np.linspace(0, size-1, size)\n",
    "            plt.scatter(x, y, s=100, c='blue', alpha=0.5)\n",
    "            \n",
    "            if i < len(self.weights):\n",
    "                # Plot connections with gradient colors\n",
    "                for j in range(size):\n",
    "                    for k in range(self.layer_sizes[i+1]):\n",
    "                        weight = self.weights[i][j, k]\n",
    "                        gradient = self.gradients[i][j, k]\n",
    "                        color = 'red' if gradient > 0 else 'blue'\n",
    "                        alpha = min(1, abs(gradient))\n",
    "                        plt.plot([i, i+1], [j, k], c=color, alpha=alpha)\n",
    "        \n",
    "        plt.title('Neural Network with Gradient Flow')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def update_gradients(self):\n",
    "        \"\"\"Simulate gradient update\"\"\"\n",
    "        self.gradients = [np.random.randn(*w.shape) for w in self.weights]\n",
    "        self.visualize_network()\n",
    "\n",
    "# Example usage\n",
    "tensor_vis = TensorVisualizer()\n",
    "neural_vis = NeuralOpsVisualizer()\n",
    "backprop_vis = BackpropVisualizer([4, 6, 4, 2])\n",
    "\n",
    "# Create some example data\n",
    "example_tensor = np.random.randn(4, 4, 4)\n",
    "tensor_vis.plot_tensor_3d(example_tensor)\n",
    "plt.show()\n",
    "\n",
    "tensor_vis.plot_tensor_slices(example_tensor)\n",
    "plt.show()\n",
    "\n",
    "print(\"Interactive Linear Layer Visualization:\")\n",
    "neural_vis.visualize_linear_layer(10, 5, batch_size=3)\n",
    "\n",
    "print(\"\\nInteractive Attention Visualization:\")\n",
    "neural_vis.visualize_attention(seq_length=8, d_k=4)\n",
    "\n",
    "print(\"\\nBackpropagation Visualization:\")\n",
    "backprop_vis.visualize_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a376b8",
   "metadata": {},
   "source": [
    "## Interactive Network Architecture Builder\n",
    "\n",
    "Build and visualize custom neural network architectures with different layer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkArchitectureBuilder:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add_layer(self, layer_type, units, activation='relu'):\n",
    "        self.layers.append({\n",
    "            'type': layer_type,\n",
    "            'units': units,\n",
    "            'activation': activation\n",
    "        })\n",
    "        self.visualize_architecture()\n",
    "    \n",
    "    def visualize_architecture(self):\n",
    "        \"\"\"Visualize current network architecture\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Calculate total height needed\n",
    "        max_units = max(layer['units'] for layer in self.layers)\n",
    "        height = max_units + 2  # Add padding\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Plot neurons\n",
    "            units = layer['units']\n",
    "            y = np.linspace(0, height-1, units)\n",
    "            plt.scatter([i]*units, y, s=100, label=f\"{layer['type']} ({units})\")\n",
    "            \n",
    "            # Add activation function annotation\n",
    "            plt.text(i, height-0.5, layer['activation'], \n",
    "                    horizontalalignment='center')\n",
    "            \n",
    "            # Draw connections to next layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                next_units = self.layers[i+1]['units']\n",
    "                next_y = np.linspace(0, height-1, next_units)\n",
    "                for y1 in y:\n",
    "                    for y2 in next_y:\n",
    "                        plt.plot([i, i+1], [y1, y2], 'gray', alpha=0.1)\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.title('Neural Network Architecture')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create interactive widgets\n",
    "layer_type = widgets.Dropdown(\n",
    "    options=['Dense', 'Conv2D', 'LSTM', 'Attention'],\n",
    "    value='Dense',\n",
    "    description='Layer Type:'\n",
    ")\n",
    "\n",
    "units = widgets.IntSlider(\n",
    "    value=32,\n",
    "    min=1,\n",
    "    max=128,\n",
    "    description='Units:'\n",
    ")\n",
    "\n",
    "activation = widgets.Dropdown(\n",
    "    options=['relu', 'tanh', 'sigmoid', 'linear'],\n",
    "    value='relu',\n",
    "    description='Activation:'\n",
    ")\n",
    "\n",
    "# Create builder instance\n",
    "builder = NetworkArchitectureBuilder()\n",
    "\n",
    "def add_layer(button):\n",
    "    builder.add_layer(layer_type.value, units.value, activation.value)\n",
    "\n",
    "add_button = widgets.Button(description='Add Layer')\n",
    "add_button.on_click(add_layer)\n",
    "\n",
    "# Display controls\n",
    "display(widgets.VBox([layer_type, units, activation, add_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6a537",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a custom tensor operation and visualize its effect\n",
    "2. Build and visualize a CNN architecture\n",
    "3. Implement and visualize different attention mechanisms\n",
    "4. Create an animation of gradient flow during training\n",
    "\n",
    "Try implementing these in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
