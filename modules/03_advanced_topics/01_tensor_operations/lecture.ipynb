{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486e1120",
   "metadata": {},
   "source": [
    "# Lecture 1: Tensor Operations for Deep Learning\n",
    "\n",
    "[![Watch the Video](https://img.shields.io/badge/Watch%20on%20YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://youtube.com/your-channel)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lecture extends our understanding from vectors and matrices to tensors, which are fundamental to deep learning operations. We'll explore tensor operations, their implementation, and their applications in neural networks.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand tensors as multi-dimensional arrays\n",
    "- Master tensor operations and broadcasting\n",
    "- Learn tensor operations in PyTorch and TensorFlow\n",
    "- Apply tensor concepts to neural network layers\n",
    "- Implement tensor operations from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "def plot_tensor_slice(tensor, slice_idx, title=\"Tensor Slice\"):\n",
    "    \"\"\"Plot a 2D slice of a 3D tensor\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(tensor[slice_idx], cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"{title} (Slice {slice_idx})\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_3d_tensor(tensor):\n",
    "    \"\"\"Create a 3D visualization of a tensor\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    x, y, z = np.indices(tensor.shape)\n",
    "    values = tensor.flatten()\n",
    "    \n",
    "    # Normalize values for color mapping\n",
    "    colors = plt.cm.viridis(values / values.max())\n",
    "    \n",
    "    ax.scatter(x.flatten(), y.flatten(), z.flatten(),\n",
    "              c=values, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Dimension 3')\n",
    "    plt.colorbar(plt.cm.ScalarMappable(cmap='viridis'))\n",
    "    plt.title('3D Tensor Visualization')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a7341c",
   "metadata": {},
   "source": [
    "## 1. Understanding Tensors\n",
    "\n",
    "A tensor is a generalization of vectors and matrices to potentially higher dimensions. The number of dimensions is called the rank of the tensor:\n",
    "\n",
    "- Scalar: Rank 0 tensor\n",
    "- Vector: Rank 1 tensor\n",
    "- Matrix: Rank 2 tensor\n",
    "- 3D array: Rank 3 tensor\n",
    "- etc.\n",
    "\n",
    "Common examples in deep learning:\n",
    "- Image batch: (batch_size, height, width, channels)\n",
    "- Text embeddings: (batch_size, sequence_length, embedding_dim)\n",
    "- Video data: (batch_size, frames, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors of different ranks\n",
    "scalar = np.array(42)\n",
    "vector = np.array([1, 2, 3])\n",
    "matrix = np.array([[1, 2], [3, 4]])\n",
    "tensor_3d = np.array([[[1, 2], [3, 4]], \n",
    "                     [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"Scalar (Rank 0):\")\n",
    "print(scalar)\n",
    "print(\"\\nVector (Rank 1):\")\n",
    "print(vector)\n",
    "print(\"\\nMatrix (Rank 2):\")\n",
    "print(matrix)\n",
    "print(\"\\nTensor 3D (Rank 3):\")\n",
    "print(tensor_3d)\n",
    "\n",
    "# Visualize 3D tensor\n",
    "visualize_3d_tensor(tensor_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51c7b2",
   "metadata": {},
   "source": [
    "## 2. Tensor Operations\n",
    "\n",
    "Key tensor operations include:\n",
    "1. Element-wise operations\n",
    "2. Reduction operations\n",
    "3. Tensor contraction\n",
    "4. Broadcasting\n",
    "5. Reshaping and permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example tensors\n",
    "A = np.random.randn(2, 3, 4)\n",
    "B = np.random.randn(2, 3, 4)\n",
    "\n",
    "# 1. Element-wise operations\n",
    "sum_tensor = A + B\n",
    "prod_tensor = A * B\n",
    "activation = np.maximum(A, 0)  # ReLU activation\n",
    "\n",
    "print(\"Element-wise Addition (shape):\", sum_tensor.shape)\n",
    "print(\"Element-wise Product (shape):\", prod_tensor.shape)\n",
    "print(\"ReLU Activation (shape):\", activation.shape)\n",
    "\n",
    "# 2. Reduction operations\n",
    "mean_all = np.mean(A)\n",
    "mean_axis0 = np.mean(A, axis=0)\n",
    "sum_axis1 = np.sum(A, axis=1)\n",
    "\n",
    "print(\"\\nMean (all):\", mean_all)\n",
    "print(\"Mean (axis 0) shape:\", mean_axis0.shape)\n",
    "print(\"Sum (axis 1) shape:\", sum_axis1.shape)\n",
    "\n",
    "# 3. Tensor contraction (matrix multiplication)\n",
    "C = np.tensordot(A, B, axes=([2], [2]))\n",
    "print(\"\\nTensor contraction shape:\", C.shape)\n",
    "\n",
    "# 4. Broadcasting\n",
    "D = A + np.random.randn(1, 3, 1)  # Broadcasting in action\n",
    "print(\"\\nBroadcasting result shape:\", D.shape)\n",
    "\n",
    "# 5. Reshaping and permutation\n",
    "E = np.transpose(A, (1, 0, 2))\n",
    "F = A.reshape(2, 12)\n",
    "print(\"\\nTransposed shape:\", E.shape)\n",
    "print(\"Reshaped shape:\", F.shape)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(A[0], cmap='viridis')\n",
    "plt.title('Original Tensor (First Slice)')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(activation[0], cmap='viridis')\n",
    "plt.title('ReLU Activation (First Slice)')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(E[0], cmap='viridis')\n",
    "plt.title('Transposed Tensor (First Slice)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a73c6",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Framework Implementation\n",
    "\n",
    "Let's look at how tensor operations are implemented in PyTorch and TensorFlow, and understand their differences and similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors in different frameworks\n",
    "# NumPy\n",
    "np_tensor = np.random.randn(2, 3, 4)\n",
    "\n",
    "# PyTorch\n",
    "torch_tensor = torch.randn(2, 3, 4)\n",
    "\n",
    "# TensorFlow\n",
    "tf_tensor = tf.random.normal((2, 3, 4))\n",
    "\n",
    "print(\"NumPy Tensor:\")\n",
    "print(np_tensor)\n",
    "print(\"\\nPyTorch Tensor:\")\n",
    "print(torch_tensor)\n",
    "print(\"\\nTensorFlow Tensor:\")\n",
    "print(tf_tensor)\n",
    "\n",
    "# Basic operations in each framework\n",
    "print(\"\\nBasic Operations:\")\n",
    "print(\"NumPy mean:\", np.mean(np_tensor))\n",
    "print(\"PyTorch mean:\", torch.mean(torch_tensor).item())\n",
    "print(\"TensorFlow mean:\", tf.reduce_mean(tf_tensor).numpy())\n",
    "\n",
    "# GPU Support (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch_tensor_gpu = torch_tensor.cuda()\n",
    "    print(\"\\nPyTorch GPU Tensor:\")\n",
    "    print(torch_tensor_gpu)\n",
    "\n",
    "# Gradient tracking\n",
    "torch_tensor_grad = torch.randn(2, 3, 4, requires_grad=True)\n",
    "tf_tensor_grad = tf.Variable(tf.random.normal((2, 3, 4)))\n",
    "\n",
    "print(\"\\nGradient Tracking:\")\n",
    "print(\"PyTorch grad enabled:\", torch_tensor_grad.requires_grad)\n",
    "print(\"TensorFlow variable:\", tf_tensor_grad.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413178b",
   "metadata": {},
   "source": [
    "## 4. Applications in Neural Networks\n",
    "\n",
    "Let's implement some common neural network operations using tensors:\n",
    "1. Linear layer (matrix multiplication)\n",
    "2. Convolutional layer\n",
    "3. Batch normalization\n",
    "4. Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorNeuralOps:\n",
    "    @staticmethod\n",
    "    def linear_layer(x, weights, bias):\n",
    "        \"\"\"Implement a linear layer: y = xW + b\"\"\"\n",
    "        return np.dot(x, weights) + bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_norm(x, eps=1e-5):\n",
    "        \"\"\"Implement batch normalization\"\"\"\n",
    "        mean = np.mean(x, axis=0)\n",
    "        var = np.var(x, axis=0)\n",
    "        return (x - mean) / np.sqrt(var + eps)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(queries, keys, values):\n",
    "        \"\"\"Implement scaled dot-product attention\"\"\"\n",
    "        d_k = queries.shape[-1]\n",
    "        scores = np.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        attention_weights = np.softmax(scores, axis=-1)\n",
    "        return np.matmul(attention_weights, values)\n",
    "\n",
    "# Example usage\n",
    "batch_size = 32\n",
    "input_dim = 20\n",
    "output_dim = 10\n",
    "seq_length = 15\n",
    "\n",
    "# Generate random data\n",
    "x = np.random.randn(batch_size, input_dim)\n",
    "weights = np.random.randn(input_dim, output_dim)\n",
    "bias = np.random.randn(output_dim)\n",
    "\n",
    "# Linear layer\n",
    "linear_output = TensorNeuralOps.linear_layer(x, weights, bias)\n",
    "print(\"Linear Layer Output Shape:\", linear_output.shape)\n",
    "\n",
    "# Batch normalization\n",
    "bn_output = TensorNeuralOps.batch_norm(x)\n",
    "print(\"Batch Norm Output Shape:\", bn_output.shape)\n",
    "\n",
    "# Attention mechanism\n",
    "queries = np.random.randn(batch_size, seq_length, input_dim)\n",
    "keys = np.random.randn(batch_size, seq_length, input_dim)\n",
    "values = np.random.randn(batch_size, seq_length, output_dim)\n",
    "attention_output = TensorNeuralOps.attention(queries, keys, values)\n",
    "print(\"Attention Output Shape:\", attention_output.shape)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_output[0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Attention Weights (First Batch)')\n",
    "plt.xlabel('Value Dimension')\n",
    "plt.ylabel('Query Dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546c3e4",
   "metadata": {},
   "source": [
    "## 5. Advanced Tensor Operations\n",
    "\n",
    "Let's explore some advanced tensor operations commonly used in modern deep learning:\n",
    "1. Tensor decomposition\n",
    "2. Einstein summation (einsum)\n",
    "3. Custom autograd operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eafa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstein summation example\n",
    "def einsum_examples():\n",
    "    # Matrix multiplication\n",
    "    a = np.random.randn(3, 4)\n",
    "    b = np.random.randn(4, 5)\n",
    "    c1 = np.einsum('ij,jk->ik', a, b)\n",
    "    c2 = a @ b\n",
    "    print(\"Matrix multiplication equivalent:\", np.allclose(c1, c2))\n",
    "    \n",
    "    # Batch matrix multiplication\n",
    "    batch_a = np.random.randn(10, 3, 4)\n",
    "    batch_b = np.random.randn(10, 4, 5)\n",
    "    batch_c = np.einsum('bij,bjk->bik', batch_a, batch_b)\n",
    "    print(\"Batch matrix multiplication shape:\", batch_c.shape)\n",
    "    \n",
    "    # Attention scores\n",
    "    queries = np.random.randn(8, 10, 64)  # (batch, seq_len, dim)\n",
    "    keys = np.random.randn(8, 15, 64)     # (batch, seq_len, dim)\n",
    "    scores = np.einsum('bik,bjk->bij', queries, keys)\n",
    "    print(\"Attention scores shape:\", scores.shape)\n",
    "\n",
    "einsum_examples()\n",
    "\n",
    "# Tensor decomposition example\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def tensor_decomposition_example():\n",
    "    # Create a 3D tensor\n",
    "    tensor = np.random.randn(10, 20, 30)\n",
    "    \n",
    "    # Matricize the tensor (reshape to 2D)\n",
    "    matrix = tensor.reshape(10, -1)\n",
    "    \n",
    "    # Perform SVD\n",
    "    n_components = 5\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    compressed = svd.fit_transform(matrix)\n",
    "    \n",
    "    # Reconstruct\n",
    "    reconstructed = svd.inverse_transform(compressed)\n",
    "    reconstructed = reconstructed.reshape(10, 20, 30)\n",
    "    \n",
    "    # Compute error\n",
    "    error = np.mean((tensor - reconstructed) ** 2)\n",
    "    print(f\"Reconstruction error: {error:.6f}\")\n",
    "    \n",
    "    return tensor, reconstructed\n",
    "\n",
    "original, reconstructed = tensor_decomposition_example()\n",
    "\n",
    "# Visualize original vs reconstructed\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(original[0], cmap='viridis')\n",
    "plt.title('Original (First Slice)')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(reconstructed[0], cmap='viridis')\n",
    "plt.title('Reconstructed (First Slice)')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(np.abs(original[0] - reconstructed[0]), cmap='viridis')\n",
    "plt.title('Difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f83b79",
   "metadata": {},
   "source": [
    "## 6. Practice Exercises\n",
    "\n",
    "1. Implement a custom tensor class with basic operations\n",
    "2. Create a mini neural network using only tensor operations\n",
    "3. Implement multi-head attention using einsum\n",
    "4. Experiment with different tensor decomposition methods\n",
    "\n",
    "Write your solutions in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6a387",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next lecture, we'll explore matrix calculus and backpropagation, building on our understanding of tensor operations.\n",
    "\n",
    "### Preparation for Next Lecture\n",
    "1. Review chain rule and partial derivatives\n",
    "2. Practice with PyTorch's autograd\n",
    "3. Study the backpropagation algorithm\n",
    "\n",
    "### Additional Resources\n",
    "- [Tensor Operations Visualization](../../resources/visualizations/tensor_ops.html)\n",
    "- [Deep Learning Mathematics](../../resources/cheat_sheets/deep_learning_math.pdf)\n",
    "- [Neural Network Implementation Guide](../../resources/guides/nn_implementation.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
