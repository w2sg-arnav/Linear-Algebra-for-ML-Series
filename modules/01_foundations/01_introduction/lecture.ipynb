{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f78a9da",
   "metadata": {},
   "source": [
    "# Lecture 1: What is Linear Algebra & Why it Matters for ML?\n",
    "\n",
    "[![Watch the Video](https://img.shields.io/badge/Watch%20on%20YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://youtube.com/your-channel)\n",
    "\n",
    "Welcome to the first lecture in our comprehensive series on Linear Algebra for Machine Learning! In this foundational lecture, we'll explore why linear algebra is the language of machine learning and how it allows us to represent and manipulate data efficiently.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the fundamental building blocks: scalars, vectors, matrices, and tensors\n",
    "- Learn how real-world data maps to linear algebra constructs\n",
    "- Build intuition for why linearity is powerful in ML\n",
    "- Set up our Python environment for the series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf4903",
   "metadata": {},
   "source": [
    "## 1. The Building Blocks of Linear Algebra\n",
    "\n",
    "Linear algebra provides us with powerful tools to represent and manipulate data. Let's start with the fundamental building blocks:\n",
    "\n",
    "1. **Scalars**: Single numbers (e.g., temperature, price)\n",
    "2. **Vectors**: Ordered lists of numbers (e.g., features of a house)\n",
    "3. **Matrices**: 2D arrays of numbers (e.g., multiple houses' features)\n",
    "4. **Tensors**: Higher-dimensional arrays (e.g., color images, video data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4792c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's set up our Python environment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97a18f",
   "metadata": {},
   "source": [
    "## 2. Real-World Examples in Machine Learning\n",
    "\n",
    "Let's see how different types of data naturally map to linear algebra constructs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: A scalar (single number)\n",
    "temperature = 25.5\n",
    "print(\"Scalar (Temperature):\", temperature)\n",
    "\n",
    "# Example 2: A vector (house features)\n",
    "house_features = np.array([\n",
    "    1500,    # Square footage\n",
    "    3,       # Bedrooms\n",
    "    2,       # Bathrooms\n",
    "    1990,    # Year built\n",
    "    350_000  # Price\n",
    "])\n",
    "print(\"\\nVector (House Features):\", house_features)\n",
    "print(\"Vector shape:\", house_features.shape)\n",
    "\n",
    "# Example 3: A matrix (multiple houses)\n",
    "houses_data = np.array([\n",
    "    [1500, 3, 2, 1990, 350_000],  # House 1\n",
    "    [2000, 4, 3, 2000, 450_000],  # House 2\n",
    "    [1200, 2, 1, 1975, 250_000]   # House 3\n",
    "])\n",
    "print(\"\\nMatrix (Multiple Houses):\")\n",
    "print(houses_data)\n",
    "print(\"Matrix shape:\", houses_data.shape)\n",
    "\n",
    "# Example 4: A simple tensor (grayscale image)\n",
    "simple_image = np.zeros((8, 8))  # 8x8 black square\n",
    "simple_image[2:6, 2:6] = 1      # White square in middle\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(simple_image, cmap='gray')\n",
    "plt.title('Simple 8x8 Image')\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(simple_image, annot=True, fmt='.0f', cmap='gray')\n",
    "plt.title('Image as Matrix Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77b23d",
   "metadata": {},
   "source": [
    "## 3. The Power of Linearity\n",
    "\n",
    "Why is linear algebra so fundamental to machine learning? The key lies in the concept of **linearity**. A transformation is linear if it satisfies two properties:\n",
    "\n",
    "1. **Additivity**: $f(x + y) = f(x) + f(y)$\n",
    "2. **Scaling**: $f(cx) = cf(x)$ for any scalar $c$\n",
    "\n",
    "These properties make linear transformations:\n",
    "- Easy to compute\n",
    "- Easy to analyze\n",
    "- Easy to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize linear transformations\n",
    "def plot_linear_transformation(transformation_matrix):\n",
    "    # Create a grid of points\n",
    "    x = np.linspace(-2, 2, 5)\n",
    "    y = np.linspace(-2, 2, 5)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    points = np.column_stack((X.flatten(), Y.flatten()))\n",
    "    \n",
    "    # Apply transformation\n",
    "    transformed_points = points @ transformation_matrix.T\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original points\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(points[:, 0], points[:, 1], c='blue', alpha=0.6)\n",
    "    plt.grid(True)\n",
    "    plt.title('Original Grid')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Transformed points\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(transformed_points[:, 0], transformed_points[:, 1], c='red', alpha=0.6)\n",
    "    plt.grid(True)\n",
    "    plt.title('Transformed Grid')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example: Rotation matrix (45 degrees)\n",
    "theta = np.pi/4  # 45 degrees\n",
    "rotation_matrix = np.array([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta), np.cos(theta)]\n",
    "])\n",
    "\n",
    "plot_linear_transformation(rotation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3790f0",
   "metadata": {},
   "source": [
    "## 4. Common Machine Learning Applications\n",
    "\n",
    "Here are some key areas where linear algebra is essential in machine learning:\n",
    "\n",
    "1. **Neural Networks**: Every layer applies linear transformations (matrix multiplication) followed by non-linear activation functions\n",
    "2. **Dimensionality Reduction**: PCA uses eigenvalues and eigenvectors to find important features\n",
    "3. **Computer Vision**: Image transformations, convolutions, and feature extraction\n",
    "4. **Natural Language Processing**: Word embeddings represent words as vectors\n",
    "5. **Recommendation Systems**: Matrix factorization for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: A neural network layer\n",
    "def simple_neural_layer(input_vector, weight_matrix, bias_vector):\n",
    "    \"\"\"Implements: output = activation(Wâ‹…x + b)\"\"\"\n",
    "    # Linear transformation\n",
    "    linear_output = np.dot(weight_matrix, input_vector) + bias_vector\n",
    "    # Non-linear activation (using ReLU)\n",
    "    output = np.maximum(0, linear_output)\n",
    "    return output\n",
    "\n",
    "# Example with small random weights\n",
    "input_size = 3\n",
    "output_size = 2\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W = np.random.randn(output_size, input_size)\n",
    "b = np.random.randn(output_size)\n",
    "x = np.array([1.0, 0.5, -0.5])\n",
    "\n",
    "print(\"Input vector shape:\", x.shape)\n",
    "print(\"Weight matrix shape:\", W.shape)\n",
    "print(\"Bias vector shape:\", b.shape)\n",
    "print(\"\\nLayer output:\", simple_neural_layer(x, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4882c4",
   "metadata": {},
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "1. Create a vector representing a student's grades in 5 subjects\n",
    "2. Create a matrix containing grades for 3 students\n",
    "3. Calculate the average grade for each student (hint: use `np.mean()`)\n",
    "4. Try creating and visualizing a simple 3x3 transformation matrix\n",
    "\n",
    "Write your solutions in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8411803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb4296",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next lecture, we'll dive deeper into vectors and develop a strong geometric intuition for vector operations. We'll see how this geometric understanding helps us reason about high-dimensional data in machine learning.\n",
    "\n",
    "### Preparation for Next Lecture\n",
    "1. Practice creating and manipulating vectors in NumPy\n",
    "2. Review basic vector operations (addition, subtraction, scaling)\n",
    "3. Think about how data in your field can be represented as vectors\n",
    "\n",
    "### Additional Resources\n",
    "- [Linear Algebra Review (PDF)](../resources/linear_algebra_review.pdf)\n",
    "- [NumPy Documentation](https://numpy.org/doc/stable/reference/routines.linalg.html)\n",
    "- [3Blue1Brown: Essence of Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
