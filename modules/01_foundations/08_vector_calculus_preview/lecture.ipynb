{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97acbdf7",
   "metadata": {},
   "source": [
    "# Lecture 8: Vector Calculus Preview for Machine Learning\n",
    "\n",
    "[![Watch the Video](https://img.shields.io/badge/Watch%20on%20YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://youtube.com/your-channel)\n",
    "\n",
    "This lecture provides a preview of vector calculus concepts that are essential for understanding optimization in machine learning.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand derivatives in multiple dimensions\n",
    "- Learn about gradients and their geometric interpretation\n",
    "- Preview optimization concepts\n",
    "- Connect vector calculus to machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_gradient_field(f, grad_f, x_range, y_range, points=20):\n",
    "    \"\"\"Plot a function and its gradient field\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], points)\n",
    "    y = np.linspace(y_range[0], y_range[1], points)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Calculate function values\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    U, V = grad_f(X, Y)\n",
    "    \n",
    "    # Normalize gradients for better visualization\n",
    "    norm = np.sqrt(U**2 + V**2)\n",
    "    U = U / (norm + 1e-6)\n",
    "    V = V / (norm + 1e-6)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Surface plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_zlabel('f(x,y)')\n",
    "    ax1.set_title('Function Surface')\n",
    "    \n",
    "    # Gradient field plot\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.quiver(X, Y, U, V)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.set_title('Gradient Field')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, (ax1, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3128ee",
   "metadata": {},
   "source": [
    "## 1. From Single to Multiple Variables\n",
    "\n",
    "In machine learning, we often work with functions of many variables. The transition from single-variable to multi-variable calculus introduces new concepts:\n",
    "\n",
    "1. Partial derivatives\n",
    "2. Gradients\n",
    "3. Directional derivatives\n",
    "4. Chain rule in multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple quadratic function\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def grad_f(x, y):\n",
    "    return 2*x, 2*y  # [∂f/∂x, ∂f/∂y]\n",
    "\n",
    "# Visualize function and its gradients\n",
    "x_range = (-2, 2)\n",
    "y_range = (-2, 2)\n",
    "\n",
    "plot_gradient_field(f, grad_f, x_range, y_range)\n",
    "plt.show()\n",
    "\n",
    "# Calculate partial derivatives at a point\n",
    "point = (1, 1)\n",
    "print(f\"At point {point}:\")\n",
    "print(f\"∂f/∂x = {2*point[0]}\")\n",
    "print(f\"∂f/∂y = {2*point[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169c91e",
   "metadata": {},
   "source": [
    "## 2. The Gradient\n",
    "\n",
    "The gradient is a vector of partial derivatives:\n",
    "\n",
    "$\\nabla f = \\begin{bmatrix} \n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Key properties:\n",
    "1. Points in direction of steepest increase\n",
    "2. Perpendicular to level curves/surfaces\n",
    "3. Magnitude indicates rate of change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657788ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: More complex function\n",
    "def g(x, y):\n",
    "    return np.sin(x) * np.cos(y)\n",
    "\n",
    "def grad_g(x, y):\n",
    "    return (np.cos(x) * np.cos(y), -np.sin(x) * np.sin(y))\n",
    "\n",
    "# Visualize\n",
    "plot_gradient_field(g, grad_g, (-np.pi, np.pi), (-np.pi, np.pi))\n",
    "plt.show()\n",
    "\n",
    "# Plot level curves with gradient vectors\n",
    "x = np.linspace(-np.pi, np.pi, 20)\n",
    "y = np.linspace(-np.pi, np.pi, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = g(X, Y)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=20)\n",
    "U, V = grad_g(X, Y)\n",
    "plt.quiver(X, Y, U, V)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Level Curves and Gradient Vectors')\n",
    "plt.colorbar(label='Function Value')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225dea8",
   "metadata": {},
   "source": [
    "## 3. Optimization Preview\n",
    "\n",
    "In machine learning, we often need to minimize or maximize functions. The gradient helps us find:\n",
    "1. Local minima and maxima\n",
    "2. Saddle points\n",
    "3. Directions of steepest descent/ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef03e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Gradient descent visualization\n",
    "def h(x, y):\n",
    "    return (x**2 + y**2) * np.exp(-0.1*(x**2 + y**2))\n",
    "\n",
    "def grad_h(x, y):\n",
    "    # Compute gradients analytically\n",
    "    dx = 2*x * np.exp(-0.1*(x**2 + y**2)) - 0.2*x*(x**2 + y**2) * np.exp(-0.1*(x**2 + y**2))\n",
    "    dy = 2*y * np.exp(-0.1*(x**2 + y**2)) - 0.2*y*(x**2 + y**2) * np.exp(-0.1*(x**2 + y**2))\n",
    "    return dx, dy\n",
    "\n",
    "# Generate surface data\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = h(X, Y)\n",
    "\n",
    "# Perform gradient descent\n",
    "def gradient_descent(start_point, learning_rate=0.1, num_steps=50):\n",
    "    path = [start_point]\n",
    "    point = np.array(start_point)\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        grad = np.array(grad_h(point[0], point[1]))\n",
    "        point = point - learning_rate * grad\n",
    "        path.append(point)\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Run gradient descent from different starting points\n",
    "start_points = [\n",
    "    (-2, -2),\n",
    "    (2, 2),\n",
    "    (-2, 2),\n",
    "    (2, -2)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Surface plot with paths\n",
    "ax1 = plt.subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "for start in start_points:\n",
    "    path = gradient_descent(start)\n",
    "    ax1.plot(path[:, 0], path[:, 1], h(path[:, 0], path[:, 1]), 'r.-')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('Gradient Descent Paths (3D)')\n",
    "\n",
    "# Contour plot with paths\n",
    "ax2 = plt.subplot(122)\n",
    "plt.contour(X, Y, Z, levels=20)\n",
    "for start in start_points:\n",
    "    path = gradient_descent(start)\n",
    "    plt.plot(path[:, 0], path[:, 1], 'r.-')\n",
    "plt.colorbar(label='Function Value')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Descent Paths (Contour)')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5824f",
   "metadata": {},
   "source": [
    "## 4. The Chain Rule in Multiple Dimensions\n",
    "\n",
    "The chain rule is crucial for:\n",
    "1. Backpropagation in neural networks\n",
    "2. Computing gradients of composite functions\n",
    "3. Understanding error propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple neural network layer\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Forward pass\n",
    "x = np.array([1, 2])  # Input\n",
    "W = np.array([[0.1, 0.2],\n",
    "              [0.3, 0.4]])  # Weights\n",
    "b = np.array([0.1, 0.2])  # Biases\n",
    "\n",
    "# Compute forward pass\n",
    "z = x @ W + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "# Backward pass (chain rule)\n",
    "da = np.array([0.5, 0.5])  # Gradient from next layer\n",
    "dz = da * d_sigmoid(z)\n",
    "dW = np.outer(x, dz)\n",
    "db = dz\n",
    "dx = dz @ W.T\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"z = {z}\")\n",
    "print(f\"a = {a}\")\n",
    "print(\"\\nBackward pass (gradients):\")\n",
    "print(f\"dW =\\n{dW}\")\n",
    "print(f\"db = {db}\")\n",
    "print(f\"dx = {dx}\")\n",
    "\n",
    "# Visualize the computation graph\n",
    "from graphviz import Digraph\n",
    "dot = Digraph(comment='Computation Graph')\n",
    "dot.attr(rankdir='LR')\n",
    "\n",
    "# Add nodes\n",
    "dot.node('x', 'x')\n",
    "dot.node('W', 'W')\n",
    "dot.node('b', 'b')\n",
    "dot.node('z', 'z = Wx + b')\n",
    "dot.node('a', 'a = sigmoid(z)')\n",
    "\n",
    "# Add edges\n",
    "dot.edge('x', 'z')\n",
    "dot.edge('W', 'z')\n",
    "dot.edge('b', 'z')\n",
    "dot.edge('z', 'a')\n",
    "\n",
    "# Display graph\n",
    "dot.render('computation_graph', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49de3af",
   "metadata": {},
   "source": [
    "## 5. Applications in Machine Learning\n",
    "\n",
    "Vector calculus is fundamental in:\n",
    "\n",
    "1. **Optimization Algorithms**\n",
    "   - Gradient descent\n",
    "   - Momentum methods\n",
    "   - Adam optimizer\n",
    "\n",
    "2. **Neural Networks**\n",
    "   - Backpropagation\n",
    "   - Weight updates\n",
    "   - Learning rate scheduling\n",
    "\n",
    "3. **Model Evaluation**\n",
    "   - Loss function gradients\n",
    "   - Performance metrics\n",
    "   - Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different optimization methods on a simple loss landscape\n",
    "def loss_surface(x, y):\n",
    "    return 0.1 * (x**2 + y**2) + np.sin(x) * np.cos(y)\n",
    "\n",
    "def grad_loss(x, y):\n",
    "    dx = 0.2 * x + np.cos(x) * np.cos(y)\n",
    "    dy = 0.2 * y - np.sin(x) * np.sin(y)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Different optimization methods\n",
    "def gradient_descent_momentum(start, lr=0.1, momentum=0.9, steps=50):\n",
    "    path = [np.array(start)]\n",
    "    velocity = np.zeros(2)\n",
    "    point = np.array(start)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        grad = grad_loss(point[0], point[1])\n",
    "        velocity = momentum * velocity - lr * grad\n",
    "        point = point + velocity\n",
    "        path.append(point)\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Compare different optimization paths\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = loss_surface(X, Y)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Standard gradient descent\n",
    "plt.subplot(131)\n",
    "plt.contour(X, Y, Z, levels=20)\n",
    "path_gd = gradient_descent((-3, 3), learning_rate=0.1)\n",
    "plt.plot(path_gd[:, 0], path_gd[:, 1], 'r.-', label='Path')\n",
    "plt.title('Standard Gradient Descent')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.colorbar(label='Loss')\n",
    "\n",
    "# Momentum\n",
    "plt.subplot(132)\n",
    "plt.contour(X, Y, Z, levels=20)\n",
    "path_momentum = gradient_descent_momentum((-3, 3))\n",
    "plt.plot(path_momentum[:, 0], path_momentum[:, 1], 'r.-', label='Path')\n",
    "plt.title('Gradient Descent with Momentum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.colorbar(label='Loss')\n",
    "\n",
    "# Loss history comparison\n",
    "plt.subplot(133)\n",
    "loss_gd = [loss_surface(x, y) for x, y in path_gd]\n",
    "loss_momentum = [loss_surface(x, y) for x, y in path_momentum]\n",
    "plt.plot(loss_gd, label='Standard GD')\n",
    "plt.plot(loss_momentum, label='Momentum')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea8c88",
   "metadata": {},
   "source": [
    "## 6. Practice Exercises\n",
    "\n",
    "1. Implement gradient descent from scratch\n",
    "2. Visualize gradients for different loss functions\n",
    "3. Implement backpropagation for a simple neural network\n",
    "4. Compare different optimization methods\n",
    "\n",
    "Write your solutions in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f22a8",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In Module 2, we'll dive deep into matrices as transformations and their applications in machine learning.\n",
    "\n",
    "### Preparation for Next Module\n",
    "1. Review matrix operations\n",
    "2. Practice with gradients and optimization\n",
    "3. Think about how transformations can be represented by matrices\n",
    "\n",
    "### Additional Resources\n",
    "- [Interactive Gradient Visualization](../../resources/visualizations/gradients.html)\n",
    "- [Vector Calculus Cheat Sheet](../../resources/cheat_sheets/vector_calculus.pdf)\n",
    "- [3Blue1Brown: Derivatives and Gradients](https://www.3blue1brown.com/lessons/derivatives)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
